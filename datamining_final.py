# -*- coding: utf-8 -*-
"""datamining_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RDReusQRQNnlofyzYFZrfmWT2alWYDox
"""

# Commented out IPython magic to ensure Python compatibility.
# %%javascript
# $.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sns
sns.set_style('whitegrid')
import tensorflow as tf
from sklearn.model_selection import train_test_split, cross_val_predict
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.decomposition import PCA
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
import time
import warnings
from sklearn.preprocessing import MaxAbsScaler
warnings.filterwarnings("ignore", category=DeprecationWarning)
# %matplotlib inline

SMALL_SIZE = 10
MEDIUM_SIZE = 12

plt.rc('font', size=SMALL_SIZE)
plt.rc('axes', titlesize=MEDIUM_SIZE)
plt.rc('axes', labelsize=MEDIUM_SIZE)
plt.rcParams['figure.dpi']=150

!pip install -U seaborn
!pip install seaborn --upgrade

df_train = pd.read_csv('train.csv')
#讀取train和test檔案
df_test = pd.read_csv('test.csv')

df_train.head()
#看看檔案對不對

df_train.info()
#可以得出dataset attribute的資訊，例如他是用什麼資料型別的

df_train.describe()


#NaN_col_names = [''] #看看有沒有哪幾項缺失或奇怪的資料
#medians = df_train.median() 
#df_train = df_train.fillna(medians)

df_train['label'].value_counts()
#星系（59%）,類星體（18%），恆星（23%）

df_train.columns.values

fig,axes = plt.subplots(nrows=1,ncols=3,figsize=(16, 4))
ax = sns.distplot(df_train[df_train['label']==2].redshift, bins = 30, ax = axes[0], kde = False)
ax.set_title('Star')
ax = sns.distplot(df_train[df_train['label']==0].redshift, bins = 30, ax = axes[1], kde = False)
ax.set_title('Galaxy')
ax = sns.distplot(df_train[df_train['label']==1].redshift, bins = 30, ax = axes[2], kde = False)
ax = ax.set_title('QSO')
#通過這張圖片，我們可以清楚的看出不同類別的紅移值差異很大。

fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(16, 4))
fig.set_dpi(100)
ax = sns.heatmap(df_train[df_train['label']==2][['u', 'g', 'r', 'i', 'z']].corr(), ax = axes[0], cmap='coolwarm')
ax.set_title('Star')
ax = sns.heatmap(df_train[df_train['label']==0][['u', 'g', 'r', 'i', 'z']].corr(), ax = axes[1], cmap='coolwarm')
ax.set_title('Galaxy')
ax = sns.heatmap(df_train[df_train['label']==1][['u', 'g', 'r', 'i', 'z']].corr(), ax = axes[2], cmap='coolwarm')
ax = ax.set_title('QSO')
#在右上角，我們觀察到每個類的相關矩陣看起來都非常相似。

df_train.drop(['obj_ID', 'run_ID', 'cam_col', 'field_ID', 'spec_obj_ID','alpha','delta','plate','MJD','fiber_ID'], axis=1, inplace=True)
df_test.drop(['obj_ID', 'run_ID', 'cam_col', 'field_ID', 'spec_obj_ID','alpha','delta','plate','MJD','fiber_ID'], axis=1, inplace=True)
df_submission=pd.DataFrame({'id': df_test.id})
df_train.head(1)

#對UGRIZ做降維，降到3維
df_train_fe = df_train
#一開始做此前處理，但是發現結果分數反而變成90


# Principal Component Analysis
#pca = PCA(n_components=3)
#ugriz = pca.fit_transform(df_train_fe[['u', 'g', 'r', 'i', 'z']])
# update dataframe 
#df_train_fe = pd.concat((df_train_fe, pd.DataFrame(ugriz)), axis=1)
#df_train_fe.rename({0: 'PCA_1', 1: 'PCA_2', 2: 'PCA_3'}, axis=1, inplace = True)
#df_train_fe.drop(['u', 'g', 'r', 'i', 'z'], axis=1, inplace=True)
#df_train_fe.head()

df_test_fe = df_test
# encode class labels to integers

# Principal Component Analysis
#pca = PCA(n_components=3)
#ugriz = pca.fit_transform(df_test_fe[['u', 'g', 'r', 'i', 'z']])

# update dataframe 
#df_test_fe = pd.concat((df_test_fe, pd.DataFrame(ugriz)), axis=1)
#df_test_fe.rename({0: 'PCA_1', 1: 'PCA_2', 2: 'PCA_3'}, axis=1, inplace = True)
#df_test_fe.drop(['u', 'g', 'r', 'i', 'z'], axis=1, inplace=True)
#df_test_fe.head()

#scaler = MinMaxScaler()
#scaler = StandardScaler()
##在MinMaxScaler中是給定了一個明確的最大值與最小值。
#每個特徵中的最小值變成了0，最大值變成了1。數據會縮放到到[0,1]之間。
#做了之後分數反而變成了更低的0.90946
df_transform_train = df_train_fe.drop('label',axis=1)
df_transform_test = df_test_fe

#df_transform_train = scaler.fit_transform(df_train_fe.drop('label', axis=1))
#df_transform_test = scaler.fit_transform(df_test_fe)

X_train, X_test, y_train, y_test = train_test_split(df_transform_train, df_train_fe['label'], test_size=0.33)
print (len(X_train), len(X_test))
##X_train= df_transform_train
#y_train = df_transform_train
#X_test = df_transform_test
#y_test = df_transform_test##
#y_train.head()
#y_test.head()

xgb = XGBClassifier(n_estimators=100)
training_start = time.perf_counter()
xgb.fit(X_train, y_train)
training_end = time.perf_counter()
prediction_start = time.perf_counter()
preds = xgb.predict(X_test)
prediction_end = time.perf_counter()
acc_xgb = (preds == y_test).sum().astype(float) / len(preds)*100
xgb_train_time = training_end-training_start
xgb_prediction_time = prediction_end-prediction_start
print("XGBoost's prediction accuracy is: %3.2f" % (acc_xgb))
print("Time consumed for training: %4.3f" % (xgb_train_time))
print("Time consumed for prediction: %6.5f seconds" % (xgb_prediction_time))

knn = KNeighborsClassifier()
training_start = time.perf_counter()
knn.fit(X_train, y_train)
training_end = time.perf_counter()
prediction_start = time.perf_counter()
preds = knn.predict(X_test)
prediction_end = time.perf_counter()
acc_knn = (preds == y_test).sum().astype(float) / len(preds)*100
knn_train_time = training_end-training_start
knn_prediction_time = prediction_end-prediction_start
print("Scikit-Learn's K Nearest Neighbors Classifier's prediction accuracy is: %3.2f" % (acc_knn))
print("Time consumed for training: %4.3f seconds" % (knn_train_time))
print("Time consumed for prediction: %6.5f seconds" % (knn_prediction_time))

scaler_gnb = MaxAbsScaler()
df_transform_train = scaler_gnb.fit_transform(df_train_fe.drop('label', axis=1))
X_train_gnb, X_test_gnb, y_train_gnb, y_test_gnb = train_test_split(df_transform_train, df_train_fe['label'], test_size=0.33)

gnb = GaussianNB()
training_start = time.perf_counter()
gnb.fit(X_train_gnb, y_train_gnb)
training_end = time.perf_counter()
prediction_start = time.perf_counter()
preds = gnb.predict(X_test_gnb)
prediction_end = time.perf_counter()
acc_gnb = (preds == y_test_gnb).sum().astype(float) / len(preds)*100
gnb_train_time = training_end-training_start
gnb_prediction_time = prediction_end-prediction_start
print("Scikit-Learn's Gaussian Naive Bayes Classifier's prediction accuracy is: %3.2f" % (acc_gnb))
print("Time consumed for training: %4.3f seconds" % (gnb_train_time))
print("Time consumed for prediction: %6.5f seconds" % (gnb_prediction_time))

result = xgb.predict(df_transform_test)
#result =  knn.predict(df_transform_test)
#result = gnb.predict(df_transform_test)
df_submission['label'] = result
df_submission.to_csv('40847007S_submission.csv', index=False)